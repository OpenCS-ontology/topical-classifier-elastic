



def get_data():
    return {
        'data': 
        [
            {
                'article_title': "Brain Tumor Detection from MRI using Adaptive Thresholding and Histogram based Techniques",
                'article_abstract': "This paper depicts a computerized framework that can distinguish brain tumor and investigate the diverse highlights of the tumor. Brain tumor segmentation means to isolated the unique tumor tissues, for example, active cells, edema and necrotic center from ordinary mind tissues of WM, GM, and CSF. However, manual segmentation in magnetic resonance data is a timeconsuming task. We present a method of automatic tumor segmentation in magnetic resonance images which consists of several steps. The recommended framework is helped by image processing based technique that gives improved precision rate of the cerebrum tumor location along with the computation of tumor measure. In this paper, the location of brain tumor from MRI is recognized utilizing adaptive thresholding with a level set and a morphological procedure with histogram. Automatic brain tumor stage is performed by using ensemble classification. Such phase classifies brain images into tumor and non-tumors using Feed Forwarded Artificial neural network based classifier. For test investigation, continuous MRI images gathered from 200 people are utilized. The rate of fruitful discovery through the proposed procedure is 97.32 percentage accurate.",
            },
            {
                'article_title': "Introduction to the Special Issue on Role of Scalable Computing and Data Analytics in Evolution of Internet of Things",
                'article_abstract': """The evolution of Internet of Things has given way to a Smart World where there is an improved integration of devices, systems and processes in humans through all pervasive connectivity. Anytime, anywhere connection and transaction is the motto of the Internet of things which brings comfort to the users and sweeps the problem of physical boundary out of the way. Once it has come into the purview of developers, new areas have been identified and new applications have been introduced. Small wearables which can track your health to big automated vehicles which can move from one place to another self navigating without human intervention are the order of the day. This has also brought into existence a new technology called cloud, since with IoT comes a large number of devices connected to the internet continuously pumping data into the cloud for storage and processing. Another area benefited from the evolution of IoT is the wireless and wired connectivity through a wide range of connectivity standards. As with any technology, it has also created a lot of concerns regarding the security, privacy and ethics. Data protection issues created by new technologies are a threat which has been recognized by developers, public and also the governing body long back. The complexity of the system arises because of the various sensors and technologies which clearly tell the pattern of the activities of the individual as well an organization making us threat prone. Moreover, the volume of the data in the cloud makes it too difficult to recognize the privacy requirement of the data or to segregate open data from private data. Data analytics is another technology which supposedly increases the opportunity of increasing business by studying this private data collected from IoT and exploring ways to monetize them. It also helps the individual by recognizing their priorities and narrowing their search. But the data collected are real world data and aggregation of this data in the cloud is an open invitation to the hackers to study about the behaviors of the individuals. The special issues of Scalable Computing has attract related to the Role of Scalable Computing and Data Analytics in Evolution of Internet of Things has attracted 28 submissions from which were selected 12."""
            },
            {
                'article_title': "An Ensemble Integrated Security System with Cross Breed Algorithm",
                'article_abstract': """Blockchain and IoT are two technologies are most widely popular in present scenario, but technologies are more complicated. The blockchain used to transforms storage and data analysis. In recent years, the blockchain is at the heart of computer technologies. It is a cryptographically secure distributed database technology for storing and transmitting information. Various attacks are done in many networks. Many research articles discussed about the security issues over the IoT based secure using block chain technology. In this paper, an Ensemble Integrated Security System (EISS) is introduced to improve the security for the heterogeneous network which consists of normal and abnormal nodes which is processed with the block chain, IoT. Results show the performance of the OUATH-2 and EISS algorithm."""
            },
            {
                'article_title': "Background Modelling using a Q-Tree Based Foreground Segmentation",
                'article_abstract': """Background modelling is an empirical part in the procedure of foreground mining of idle and moving objects. The foreground object detection has become a challenging phenomenon due to intermittent objects, intensity variation, image artefact and dynamic background in the video analysis and video surveillance applications. In the video surveillances application, a large amount of data is getting processed by everyday basis. Thus it needs an efficient background modelling technique which could process those larger sets of data which promotes effective foreground detection. In this paper, we presented a renewed background modelling method for foreground segmentation. The main objective of the work is to perform the foreground extraction only in the intended region of interest using proposed Q-Tree algorithm. At most all the present techniques consider their updates to the pixels of the entire frame which may result in inefficient foreground detection with a quick update to slow moving objects. The proposed method contract these defect by extracting the foreground object by controlling the region of interest (the region only where the background subtraction is to be performed) and thereby reducing the false positive and false negative. The extensive experimental results and the evaluation parameters of the proposed approach with the state of art method were compared against the most recent background subtraction approaches. Moreover, we use challenge change detection dataset and the efficiency of our method is analyzed in different environmental conditions (indoor, outdoor) from the CDnet2014 dataset and additional real time videos. The experimental results were satisfactorily verified the strengths and weakness of proposed method against the existing state-of-the-art background modelling methods."""
            },
            {
                'article_title': "Experiences with mesh-like computations using predictionbinarytrees",
                'article_abstract': """In this paper we aim at exploiting the temporal coherence among successive phases of a computation, in order to implement a load-balancing technique in mesh-like computations to be mapped on a cluster of processors. A key concept, on which the load balancing schema is built on, is the use of a Predictor component that is in charge of providing an estimation of the unbalancing between successive phases. By using this information, our method partitions the computation in balanced tasks through the Prediction Binary Tree (PBT). At each new phase,current PBT is updated by using previous phase computing time for each task as next-phaseâ€™s cost estimate. The PBT is designed so that it balances the load across the tasks as well as reduces dependencyamong processors for higher performances. Reducing dependency is obtained by using rectangular tiles of the mesh, of almost-square shape (i. e. one dimension is at most twice the other). By reducing dependency, one can reduce inter-processors communication or exploit local dependencies among tasks (such as data locality). Furthermore, we also provide two heuristics which take advantage of data-locality. Our strategy has been assessed on a significant problem, Parallel Ray Tracing. Our implementation shows a good scalability, and improves performance in both cheaper commodity cluster and high performance clusters with low latency networks. We report different measurements showingthat tasks granularity is a key point for the performances of our decomposition/mapping strategy."""
            },
            {
                'article_title': "Map-reduce based distance weighted k-nearest neighbor machinelearning algorithm for big data applications",
                'article_abstract': """With the evolution of Internet standards and advancements in various Internet and mobile technologies, especially since web 4.0, more and more web and mobile applications emerge such as e-commerce, social networks, online gaming applications and Internet of Things based applications. Due to the deployment and concurrent access of these applications on the Internet and mobile devices, the amount of data and the kind of data generated increases exponentially and the new era of Big Data has come into existence. Presently available data structures and data analyzing algorithms are not capable to handle such Big Data. Hence, there is a need for scalable, flexible, parallel and intelligent data analyzing algorithms to handle and analyze the complex massive data. In this article, we have proposed a novel distributed supervised machine learning algorithm based on the MapReduce programming model and Distance Weighted k-Nearest Neighbor algorithm called MR-DWkNN to process and analyze the Big Data in the Hadoop cluster environment. The proposed distributed algorithm is based on supervised learning performs both regression tasks as well as classification tasks on large-volume of Big Data applications. Three performance metrics, such as Root Mean Squared Error (RMSE), Determination coefficient (R2) for regression task, and Accuracy for classification tasks are utilized for the performance measure of the proposed MR-DWkNN algorithm. The extensive experimental results shows that there is an average increase of 3% to 4.5% prediction and classification performances as compared to standard distributed k-NN algorithm and a considerable decrease of Root Mean Squared Error (RMSE) with good parallelism characteristics of scalability and speedup thus, proves its effectiveness in Big Data predictive and classification applications."""
            },
            {
                'article_title': "Static load balancing technique for geographically partitioned public cloud",
                'article_abstract': """Large number of users are shifting to the cloud system for their different kind of needs. Hence the number of applications on public cloud are increasing day by day. Handling public cloud is becoming unmanageable in comparison to other counterparts. Though fog technology has reduced the load on centralized cloud resources to a remarkable extent, still load handled at cloud end is significantly high. Geographic partitioning of public cloud can resolve these issues by adding manageability and efficiency in this situation. Dividing public cloud in smaller partitions opens ways to manage resources and requests in a better way. But, partitioned clouds introduce different ends for submission and operations of tasks and virtual machines. We have tried to handle all these complexities in this paper. Proposed work is focused upon load balancing in the partitioned public cloud by combining centralized and decentralized approaches, assuming the presence of fog layer. A load balancer entity is used for decentralized load balancing at partitions and a controller entity is used for centralized level to balance the overall load at various partitions. In the proposed approach, it is assumed that jobs are segregated first. All the jobs which can be handled locally by fog resources are not forwarded to the cloud layer directly. Those are processed locally by decentralized fog resources. Selection of an appropriate Virtual Machine (VM) for filtered set of job, which are forwarded to cloud environment, is done in three steps. Initially, selecting the partition with a maximum available capacity of resources. Then finding the appropriate node with maximum available resources, within a selected partition. And finally, the VM with minimum execution time for a task is chosen. Results are compared with the results produced in this work with First Come First Serve (FCFS) and Shortest Job First (SJF) algorithms, implemented in same setup i.e. partitioned cloud. This paper compares the Waiting Time, Finish Time and Actual Run Time of tasks using these algorithms. After initial experimentation, it is found that in most of the cases, while comparing above parameters, the proposed approach is producing better results than FCFS algorithm. But results produced by SJF algorithm produce better results. To reduce the number of unhandled jobs, a new load state is introduced which checks load beyond conventional load states. Major objective of this approach is to reduce the need of runtime virtual machine migration and to reduce the wastage of resources, which may be occurring due to predefined values of threshold. The implementation is done using CloudSim."""
            }
        ]
    }